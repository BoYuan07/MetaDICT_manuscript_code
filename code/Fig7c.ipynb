{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Define the neural network model\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction of PD1 response status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets for prediction of pd1\n",
    "df = pd.read_csv(\"../result/pd1/metadict.csv\", index_col=0)\n",
    "count_metadict = df.to_numpy().T\n",
    "\n",
    "# read datasets\n",
    "df0 = pd.read_csv(\"../result/pd1/conqur.csv\", index_col=0)\n",
    "count_conqur = df0.to_numpy().T\n",
    "\n",
    "df1 = pd.read_csv(\"../result/pd1/combatseq.csv\", index_col=0)\n",
    "count_combatseq = df1.to_numpy().T\n",
    "\n",
    "df2 = pd.read_csv(\"../result/pd1/percentile.csv\", index_col=0)\n",
    "count_percentile = df2.to_numpy().T\n",
    "\n",
    "df3 = pd.read_csv(\"../result/pd1/debiasm.csv\", index_col=0)\n",
    "count_debiasm = df3.to_numpy().T\n",
    "\n",
    "df4 = pd.read_csv(\"../result/pd1/plsda.csv\", index_col=0)\n",
    "count_plsda = df4.to_numpy().T\n",
    "\n",
    "df5 = pd.read_csv(\"../result/pd1/mmuphin.csv\", index_col=0)\n",
    "count_mmuphin = df5.to_numpy().T\n",
    "\n",
    "df6 = pd.read_csv(\"../result/pd1/scanvi.csv\", index_col=0)\n",
    "count_scanvi = df6.to_numpy().T\n",
    "\n",
    "df7 = pd.read_csv(\"../result/pd1/count_otu.csv\", index_col=0)\n",
    "count_raw = df7.to_numpy().T\n",
    "\n",
    "count_dir = {\"MetaDICT\": count_metadict, \"ComBatSeq\": count_combatseq, \"DEBIAS-M\": count_debiasm, \"Percentile-Norm\": count_percentile, \n",
    "             \"PLSDA-batch\": count_plsda, \"MMUPHin\": count_mmuphin, \"Unprocessed\": count_raw, \"scANVI\": count_scanvi, \"ConQuR\": count_conqur}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadf = pd.read_csv(\"../result/pd1/meta.csv\", index_col=0)\n",
    "Response = metadf[\"Response\"].to_list()\n",
    "Y = np.array([1 if item == \"R\" else 0 for item in Response])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave-one-out\n",
    "Dataset = np.array(metadf[\"Dataset\"].to_list())\n",
    "dataset_unique = np.unique(Dataset)\n",
    "dataset_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_l = [16,64,64,64,64,64] # predefined batch size\n",
    "number_epoch = [50,50,50,50,50,50]\n",
    "roc_auc_curve = {}\n",
    "roc_auc_score = []\n",
    "test = []\n",
    "method_l = []\n",
    "for i in range(len(dataset_unique)):\n",
    "    name = dataset_unique[i]\n",
    "    roc_auc_curve[name] = {}\n",
    "    for method, count in count_dir.items():\n",
    "        split_data_training = {dataset_name: count[Dataset != dataset_name,:] for dataset_name in dataset_unique}\n",
    "        split_label_training = {dataset_name: Y[Dataset != dataset_name] for dataset_name in dataset_unique}\n",
    "\n",
    "        split_data_test = {dataset_name: count[Dataset == dataset_name,:] for dataset_name in dataset_unique}\n",
    "        split_label_test = {dataset_name: Y[Dataset == dataset_name] for dataset_name in dataset_unique}\n",
    "        \n",
    "        \n",
    "        # Convert the data to PyTorch tensors\n",
    "        torch.manual_seed(2025)\n",
    "        X_train = torch.tensor(split_data_training[name], dtype=torch.float32)\n",
    "        y_train = torch.tensor(split_label_training[name], dtype=torch.float32).view(-1, 1)\n",
    "        X_test = torch.tensor(split_data_test[name], dtype=torch.float32)\n",
    "        y_test = torch.tensor(split_label_test[name], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        # Create PyTorch datasets and loaders\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "        batch_size = batch_size_l[i]\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        model = BinaryClassifier(input_dim=X_train.shape[1])\n",
    "\n",
    "        #Define the loss function and optimizer\n",
    "        criterion = nn.BCEWithLogitsLoss() \n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        # Training loop\n",
    "        num_epochs = number_epoch[i]\n",
    "        for epoch in tqdm(range(num_epochs),desc = f\"processing\"):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()          # Zero the parameter gradients\n",
    "                outputs = model(inputs)          # Forward pass\n",
    "                loss = criterion(outputs, labels)  # Compute loss\n",
    "                loss.backward()                # Backward pass\n",
    "                optimizer.step()               # Update weights\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        #Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_test)\n",
    "            # Concatenate predictions and labels from all batches\n",
    "        fpr, tpr, _ = roc_curve(y_test, outputs)  # Compute ROC curve\n",
    "        roc_auc = auc(fpr, tpr)  # Compute AUC score\n",
    "        print(f\"{method} ROC-AUC Score: {roc_auc:.4f}\")\n",
    "        results = pd.DataFrame({\"fpr\": fpr, \"tpr\": tpr})\n",
    "        results.to_csv(f\"../result/pd1/roc_{name}_{method}.csv\", index=False)\n",
    "\n",
    "        test.append(name)\n",
    "        method_l.append(method)\n",
    "        roc_auc_score.append(roc_auc)\n",
    "\n",
    "\n",
    "        roc_auc_curve[name][method] = [fpr, tpr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_nc = {\"Method\":method_l, \"Test\": test, \"ROC-AUC\":roc_auc_score}\n",
    "df = pd.DataFrame(roc_auc_nc)\n",
    "df.to_csv(\"../result/pd1/roc_pd1_nn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure\n",
    "line_styles = ['-', '--', '-.', ':', '-', '--','solid','dashed']\n",
    "# Iterate over your curves and plot each one\n",
    "for name, all_curve in roc_auc_curve.items():\n",
    "    k = 0\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for method, curve in all_curve.items():\n",
    "        fpr = curve[0]\n",
    "        tpr = curve[1]\n",
    "        score = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'{method}: AUC = {score: .2f}',linestyle=line_styles[k],linewidth=2)\n",
    "        k += 1\n",
    "\n",
    "    \n",
    "    # Add a diagonal line for reference (random classifier)\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "\n",
    "    # Set plot labels and title\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curves ({name})\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"../fig/roc_all_{name}.jpeg\",dpi = 300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative Control experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets for negative control experiments\n",
    "df = pd.read_csv(\"../result/pd1/metadict_nc.csv\", index_col=0)\n",
    "count_metadict = df.to_numpy().T\n",
    "\n",
    "# read datasets\n",
    "df0 = pd.read_csv(\"../result/pd1/conqur_nc.csv\", index_col=0)\n",
    "count_conqur = df0.to_numpy().T\n",
    "\n",
    "df1 = pd.read_csv(\"../result/pd1/combatseq_nc.csv\", index_col=0)\n",
    "count_combatseq = df1.to_numpy().T\n",
    "\n",
    "df2 = pd.read_csv(\"../result/pd1/percentile_nc.csv\", index_col=0)\n",
    "count_percentile = df2.to_numpy().T\n",
    "\n",
    "df3 = pd.read_csv(\"../result/pd1/debiasm_nc.csv\", index_col=0)\n",
    "count_debiasm = df3.to_numpy().T\n",
    "\n",
    "df4 = pd.read_csv(\"../result/pd1/plsda_nc.csv\", index_col=0)\n",
    "count_plsda = df4.to_numpy().T\n",
    "\n",
    "df5 = pd.read_csv(\"../result/pd1/mmuphin_nc.csv\", index_col=0)\n",
    "count_mmuphin = df5.to_numpy().T\n",
    "\n",
    "df6 = pd.read_csv(\"../result/pd1/scanvi_nc.csv\", index_col=0)\n",
    "count_scanvi = df6.to_numpy().T\n",
    "\n",
    "df7 = pd.read_csv(\"../result/pd1/count_otu.csv\", index_col=0)\n",
    "count_raw = df7.to_numpy().T\n",
    "\n",
    "count_dir = {\"MetaDICT\": count_metadict, \"ComBatSeq\": count_combatseq, \"DEBIAS-M\": count_debiasm, \"Percentile-Norm\": count_percentile, \n",
    "             \"PLSDA-batch\": count_plsda, \"MMUPHin\": count_mmuphin, \"Unprocessed\": count_raw, \"scANVI\": count_scanvi, \"ConQuR\": count_conqur}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadf1 = pd.read_csv(\"../result/pd1/meta_nc.csv\", index_col=0)\n",
    "Group = metadf1[\"Y\"].to_list()\n",
    "Y1 = np.array([1 if item == \"Group 1\" else 0 for item in Group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_l = [16,64,64,64,64,64] # predefined batch size\n",
    "number_epoch = [50,50,50,50,50,50]\n",
    "roc_auc_curve = {}\n",
    "roc_auc_score = []\n",
    "test = []\n",
    "method_l = []\n",
    "for i in range(len(dataset_unique)):\n",
    "    name = dataset_unique[i]\n",
    "    roc_auc_curve[name] = {}\n",
    "    for method, count in count_dir.items():\n",
    "        split_data_training = {dataset_name: count[Dataset != dataset_name,:] for dataset_name in dataset_unique}\n",
    "        split_label_training = {dataset_name: Y1[Dataset != dataset_name] for dataset_name in dataset_unique}\n",
    "\n",
    "        split_data_test = {dataset_name: count[Dataset == dataset_name,:] for dataset_name in dataset_unique}\n",
    "        split_label_test = {dataset_name: Y1[Dataset == dataset_name] for dataset_name in dataset_unique}\n",
    "        \n",
    "        \n",
    "        # Convert the data to PyTorch tensors\n",
    "        torch.manual_seed(2025)\n",
    "        X_train = torch.tensor(split_data_training[name], dtype=torch.float32)\n",
    "        y_train = torch.tensor(split_label_training[name], dtype=torch.float32).view(-1, 1)\n",
    "        X_test = torch.tensor(split_data_test[name], dtype=torch.float32)\n",
    "        y_test = torch.tensor(split_label_test[name], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        # Create PyTorch datasets and loaders\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "        batch_size = batch_size_l[i]\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        model = BinaryClassifier(input_dim=X_train.shape[1])\n",
    "\n",
    "        #Define the loss function and optimizer\n",
    "        criterion = nn.BCEWithLogitsLoss() \n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        # Training loop\n",
    "        num_epochs = number_epoch[i]\n",
    "        for epoch in tqdm(range(num_epochs),desc = f\"processing\"):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()          # Zero the parameter gradients\n",
    "                outputs = model(inputs)          # Forward pass\n",
    "                loss = criterion(outputs, labels)  # Compute loss\n",
    "                loss.backward()                # Backward pass\n",
    "                optimizer.step()               # Update weights\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        #Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_test)\n",
    "            # Concatenate predictions and labels from all batches\n",
    "        fpr, tpr, _ = roc_curve(y_test, outputs)  # Compute ROC curve\n",
    "        roc_auc = auc(fpr, tpr)  # Compute AUC score\n",
    "        print(f\"{method} ROC-AUC Score: {roc_auc:.4f}\")\n",
    "        results = pd.DataFrame({\"fpr\": fpr, \"tpr\": tpr})\n",
    "        results.to_csv(f\"../result/pd1/roc_{name}_{method}_nc.csv\", index=False)\n",
    "        \n",
    "        test.append(name)\n",
    "        method_l.append(method)\n",
    "        roc_auc_score.append(roc_auc)\n",
    "\n",
    "\n",
    "        roc_auc_curve[name][method] = [fpr, tpr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_nc = {\"Method\":method_l, \"Test\": test, \"ROC-AUC\":roc_auc_score}\n",
    "df = pd.DataFrame(roc_auc_nc)\n",
    "df.to_csv(\"../result/pd1/roc_nc_nn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure\n",
    "line_styles = ['-', '--', '-.', ':', '-', '--']\n",
    "# Iterate over your curves and plot each one\n",
    "for name, all_curve in roc_auc_curve.items():\n",
    "    k = 0\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for method, curve in all_curve.items():\n",
    "        if method in ['MetaDICT', \"Unprocessed\"]:\n",
    "            fpr = curve[0]\n",
    "            tpr = curve[1]\n",
    "            score = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'{method}: AUC = {score: .2f}',linestyle=line_styles[k],linewidth=2)\n",
    "            k += 1\n",
    "\n",
    "    \n",
    "    # Add a diagonal line for reference (random classifier)\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "\n",
    "    # Set plot labels and title\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curves ({name})\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"../fig/roc_{name}.jpeg\",dpi = 300)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
